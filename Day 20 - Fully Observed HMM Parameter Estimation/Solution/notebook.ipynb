{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0708eadc1cb4217c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Day 20 notebook\n",
    "\n",
    "The objectives of this notebook are to practice \n",
    "\n",
    "* estimating the parameters of an HMM in the fully observed scenario\n",
    "* constructing and training an HMM for an application with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb337de81168447b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Modules used in this activity\n",
    "import math      # for log\n",
    "import functools # for reduce\n",
    "import random    # for simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a436d3c66d266c28",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## A `HiddenMarkovModel` class\n",
    "\n",
    "In this activity, we will continue to expand on the `HiddenMarkovModel` class that we developed in the previous activities.  Compared to class in the last activity, the constructor (`__init__`) of the class has been modified slightly in two ways:\n",
    "1. The parameter arguments to the constructor are now optional, defaulting to None.  When the parameters are not\n",
    "   given, the HMM will need be trained later using the `estimate_parameters` method.\n",
    "2. A helper method `_compute_log_parameters` is called to precompute the log parameters.  This method should be \n",
    "   called anytime the parameters are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenMarkovModel:\n",
    "    def __init__(self, states, chars, \n",
    "                 transition_prob_matrix=None, initial_probs=None, emission_prob_matrix=None):\n",
    "        \"\"\"Initializes a HiddenMarkovModel\n",
    "        \n",
    "        Models represented by this class do not explicitly represent a begin state and do\n",
    "        not allow for an end state.\n",
    "                \n",
    "        Args:\n",
    "            states: a string giving the characters representing the hidden states\n",
    "                of the model (1 character per state)\n",
    "            chars: a string giving the set of characters possibly emitted by the\n",
    "                states of the model\n",
    "            transition_prob_matrix: a list of lists of probabilities representing a\n",
    "                transition probability matrix. transition_prob_matrix[s][t] should equal \n",
    "                P(pi_i = t | pi_{i-1} = s). Row s is thus the conditional probability \n",
    "                distribution P(pi_i | pi_{i-1} = s). The indices in this matrix correspond \n",
    "                to the indices of the states in the states argument.  If None, then this\n",
    "                model will need to be trained via estimate_parameters.\n",
    "            initial_probs: a list of probabilities representing the initial state \n",
    "                probabilities. Entry s of this list is P(pi_1 = s), i.e., the probability that\n",
    "                the first hidden state in the chain is s.  The indices of this list correspond to the\n",
    "                indices of the states in the states argument.  If None, then this\n",
    "                model will need to be trained via estimate_parameters.\n",
    "            emission_prob_matrix: a list of lists of probabilities representing an emission\n",
    "                probability matrix.  emission_prob_matrix[s][c] should equal \n",
    "                P(X_i = c | pi_i = s), i.e., the probability of state s emitting character c. \n",
    "                Row s is thus the conditional probability distribution P(X_i | pi_i = s).\n",
    "                The row indices of this matrix correspond to the indices of the states in\n",
    "                the states argument.  The column indices of the matrix correspond to the \n",
    "                indices of the characters in the chars argument.  If None, then this\n",
    "                model will need to be trained via estimate_parameters.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.chars = chars\n",
    "        self.transition_prob_matrix = transition_prob_matrix\n",
    "        self.initial_probs = initial_probs\n",
    "        self.emission_prob_matrix = emission_prob_matrix\n",
    "        \n",
    "        # precompute log parameters if the the raw parameters were specified\n",
    "        if None not in (transition_prob_matrix, initial_probs, emission_prob_matrix):\n",
    "            self._compute_log_parameters()\n",
    "    \n",
    "    def _compute_log_parameters(self):\n",
    "        \"\"\"Computes and stores log-transformations of the model parameters.\n",
    "        \n",
    "        This method should be run whenever the parameters of the model are updated.\n",
    "        \"\"\"\n",
    "        self.log_transition_prob_matrix = log_transform_matrix(self.transition_prob_matrix)\n",
    "        self.log_initial_probs = log_transform_vector(self.initial_probs)\n",
    "        self.log_emission_prob_matrix = log_transform_matrix(self.emission_prob_matrix)\n",
    "    \n",
    "    def encode_states(self, state_sequence):\n",
    "        \"\"\"Encodes a string of state characters as a list of indices of the states.\"\"\"\n",
    "        return [self.states.index(char) for char in state_sequence]\n",
    "\n",
    "    def decode_states(self, indices):\n",
    "        \"\"\"Decodes a sequence of state indices into a string of the state characters.\"\"\"\n",
    "        return \"\".join(self.states[index] for index in indices)\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        \"\"\"Encodes a string of observed characters as a list of indices of the characters.\"\"\"\n",
    "        return [self.chars.index(char) for char in sequence]\n",
    "\n",
    "    def decode_sequence(self, indices):\n",
    "        \"\"\"Decodes a sequence of observed character indices into a string of characters.\"\"\"\n",
    "        return \"\".join(self.chars[index] for index in indices)\n",
    "\n",
    "    def estimate_parameters(self, training_data, pseudocount=0):\n",
    "        \"\"\"Estimates the parameters of the model given observed sequences and state paths.\n",
    "        \n",
    "        Computes maximum likelihood parameters for the the completely observed scenario.  \n",
    "\n",
    "        Args:\n",
    "            training_data: A list of tuples of the form (state_string, char_string) where\n",
    "                state_string is a string of state characters and char_string is a string\n",
    "                of observed characters.\n",
    "            pseudocount: a pseudocount to add to each total observed count when computing the \n",
    "                parameter values.  The default is zero, which corresponds to maximimum \n",
    "                likelihood estimates without smoothing.  A value of one for this parameter\n",
    "                corresponds to Laplace smoothing.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # initialize matrices of counts\n",
    "        transition_count_matrix = matrix(len(self.states), len(self.states), pseudocount)\n",
    "        initial_counts = [pseudocount] * len(self.states)\n",
    "        emission_count_matrix = matrix(len(self.states), len(self.chars), pseudocount)\n",
    "        \n",
    "        for state_path, sequence in training_data:\n",
    "            encoded_sequence = self.encode_sequence(sequence)\n",
    "            encoded_state_path = self.encode_states(state_path)\n",
    "            # count transitions\n",
    "            if state_path: initial_counts[encoded_state_path[0]] += 1\n",
    "            for k, l in zip(encoded_state_path, encoded_state_path[1:]):\n",
    "                transition_count_matrix[k][l] += 1\n",
    "            # count emissions\n",
    "            for k, c in zip(encoded_state_path, encoded_sequence):\n",
    "                emission_count_matrix[k][c] += 1\n",
    "        \n",
    "        # update transition parameters\n",
    "        self.initial_probs = normalize_vector(initial_counts)\n",
    "        self.transition_prob_matrix = normalize_matrix_rows(transition_count_matrix)\n",
    "        self.emission_prob_matrix = normalize_matrix_rows(emission_count_matrix)\n",
    "        \n",
    "        ### END SOLUTION\n",
    "        self._compute_log_parameters()\n",
    "    \n",
    "    def simulate(self, length):\n",
    "        \"\"\"Simulates a sequence of hidden states and emitted characters of\n",
    "        the given length from this HMM.\n",
    "        \n",
    "        Args:\n",
    "            length: the length of the sequence to simulate\n",
    "        Returns:\n",
    "            A tuple of the form (hidden_state_string, char_string) where hidden_state_string is a\n",
    "            string of state characters and char_string is a string of observed characters.\n",
    "        \"\"\"\n",
    "        state_indices = [None] * length\n",
    "        char_indices = [None] * length\n",
    "        for i in range(length):\n",
    "            state_probs = self.transition_prob_matrix[state_indices[i - 1]] if i > 0 else self.initial_probs\n",
    "            state_indices[i] = sample_categorical(state_probs)\n",
    "            char_indices[i] = sample_categorical(self.emission_prob_matrix[state_indices[i]])\n",
    "            \n",
    "        return (self.decode_states(state_indices), self.decode_sequence(char_indices))\n",
    "        \n",
    "    def log_joint_probability(self, hidden_state_string, char_string):\n",
    "        \"\"\"Calculates the (natural) log joint probability of a path of hidden states\n",
    "        and an observed sequence given this HMM.\n",
    "        \n",
    "        Args:\n",
    "            hidden_state_string: a string representing the sequence of hidden states (pi)\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            log(P(hidden_states, observed_chars))\n",
    "        \"\"\"\n",
    "        state_indices = self.encode_states(hidden_state_string)\n",
    "        char_indices = self.encode_sequence(char_string)\n",
    "\n",
    "        log_p = 0.0\n",
    "        last_state_index = None\n",
    "        for state_index, char_index in zip(state_indices, char_indices):\n",
    "            if last_state_index is None:\n",
    "                log_p += self.log_initial_probs[state_index]\n",
    "            else:\n",
    "                log_p += self.log_transition_prob_matrix[last_state_index][state_index]\n",
    "            log_p += self.log_emission_prob_matrix[state_index][char_index]\n",
    "            last_state_index = state_index\n",
    "        return log_p\n",
    "\n",
    "    def viterbi_matrix(self, char_string):\n",
    "        \"\"\"Computes the (log-transformed) Viterbi dynamic programming matrix V for\n",
    "        the given observed sequence.\n",
    "\n",
    "        Args:\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            A matrix (list of lists) representing the Viterbi dynamic programming matrix,\n",
    "            with rows corresponding to states and columns corresponding to positions in the\n",
    "            sequence.\n",
    "        \"\"\"\n",
    "        char_indices = self.encode_sequence(char_string)\n",
    "        \n",
    "        # Initialize the viterbi dynamic programming matrix\n",
    "        # the entry V[k][i] corresponds to the subproblem V_k(i+1)\n",
    "        # where i is a 0-based index (e.g., V[k][0] corresponds to the subproblem\n",
    "        # of the most probable path of the prefix of length = 1). We will not explicitly\n",
    "        # represent the begin or end states.  As a result, we will not explicitly store the\n",
    "        # initialization values described in the textbook and lecture.\n",
    "        V = matrix(len(self.states), len(char_string))\n",
    "        if not char_string: return V\n",
    "        \n",
    "        # initialization (first position in sequence)\n",
    "        for ell in range(len(self.states)):    # loop over hidden state indices\n",
    "            V[ell][0] = (self.log_initial_probs[ell] + \n",
    "                         self.log_emission_prob_matrix[ell][char_indices[0]])\n",
    "\n",
    "        # main fill stage\n",
    "        for i in range(1, len(char_string)):    # loop over positions\n",
    "            for ell in range(len(self.states)): # loop over hidden state indices\n",
    "                V[ell][i] = (self.log_emission_prob_matrix[ell][char_indices[i]] + \n",
    "                             max(V[k][i - 1] + self.log_transition_prob_matrix[k][ell]\n",
    "                                 for k in range(len(self.states))))\n",
    "\n",
    "        return V\n",
    "    \n",
    "    def viterbi_traceback(self, V):\n",
    "        \"\"\"Computes a most probable path given a (log) Viterbi dynamic programming matrix.\n",
    "        \n",
    "        Uses a traceback procedure that does not require traceback pointers.  In the case of\n",
    "        ties, this traceback prefers the state with the largest index.\n",
    "        \n",
    "        Args:\n",
    "            V: A matrix (list of lists) representing the Viterbi dynamic programming matrix\n",
    "               containing log-transformed values.\n",
    "        Returns:\n",
    "            A string representing a most probable sequence of hidden states\n",
    "        \"\"\"\n",
    "        L = len(V[0])               # deduce the length of the sequence from # columns in V\n",
    "        if L == 0: return \"\"        # empty string base case\n",
    "        state_indices = [None] * L  # initialize hidden state path\n",
    "        \n",
    "        # determine the state at the last position in a most probable path\n",
    "        max_prob, max_state = max((V[k][L - 1], k) for k in range(len(self.states)))\n",
    "        state_indices[L - 1] = max_state\n",
    "        \n",
    "        # traceback from this last state by redoing the recurrence calculation at each step\n",
    "        # the emission probabilities are not included in the calculations because they are\n",
    "        # irrelevant for determining the maximizing state\n",
    "        for i in range(L - 1, 0, -1):\n",
    "            max_prob, max_state = max((V[k][i - 1] + self.log_transition_prob_matrix[k][max_state], k)\n",
    "                                      for k in range(len(self.states)))\n",
    "            state_indices[i - 1] = max_state\n",
    "            \n",
    "        # return string representation of hidden state path\n",
    "        return self.decode_states(state_indices)        \n",
    "\n",
    "    def most_probable_path(self, char_string):\n",
    "        \"\"\"Computes a most probable path of hidden states for the observed sequence.\n",
    "\n",
    "        Args:\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            A string representing a most probable sequence of hidden states.\n",
    "        \"\"\"\n",
    "        V = self.viterbi_matrix(char_string)\n",
    "        return self.viterbi_traceback(V)    \n",
    "\n",
    "### BEGIN SOLUTION TEMPLATE=       \n",
    "    def forward_matrix(self, char_string):\n",
    "        \"\"\"Computes the (log-transformed) Forward dynamic programming matrix f for\n",
    "        the given observed sequence.\n",
    "\n",
    "        Args:\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            A matrix (list of lists) representing the Forward dynamic programming matrix,\n",
    "            with rows corresponding to states and columns corresponding to positions in the\n",
    "            sequence.\n",
    "        \"\"\"\n",
    "        char_indices = self.encode_sequence(char_string)\n",
    "        \n",
    "        # Initialize the forward dynamic programming matrix\n",
    "        # the entry f[k][i] corresponds to the subproblem f_k(i+1)\n",
    "        # where i is a 0-based index (e.g., f[k][0] corresponds to the subproblem\n",
    "        # of the probability of the prefix of length = 1 and ending in state k). We will \n",
    "        # not explicitly represent the begin or end states.  As a result, we will not\n",
    "        # explicitly store the initialization values described in the textbook and lecture.\n",
    "        f = matrix(len(self.states), len(char_string))\n",
    "        if not char_string: return f\n",
    "        \n",
    "        # initialization\n",
    "        for ell in range(len(self.states)):\n",
    "            f[ell][0] = (self.log_initial_probs[ell] +\n",
    "                         self.log_emission_prob_matrix[ell][char_indices[0]])\n",
    "\n",
    "        # main fill stage\n",
    "        for i in range(1, len(char_string)):\n",
    "            for ell in range(len(self.states)):\n",
    "                f[ell][i] = (self.log_emission_prob_matrix[ell][char_indices[i]] + \n",
    "                             sum_log_probs(f[k][i - 1] + \n",
    "                                           self.log_transition_prob_matrix[k][ell]\n",
    "                                           for k in range(len(self.states))))\n",
    "\n",
    "        return f\n",
    "    \n",
    "    def backward_matrix(self, char_string):\n",
    "        \"\"\"Computes the (log-transformed) Backward dynamic programming matrix f for\n",
    "        the given observed sequence.\n",
    "\n",
    "        Args:\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            A matrix (list of lists) representing the Backward dynamic programming matrix,\n",
    "            with rows corresponding to states and columns corresponding to positions in the\n",
    "            sequence.\n",
    "        \"\"\"\n",
    "        char_indices = self.encode_sequence(char_string)\n",
    "        \n",
    "        # Initialize the backward dynamic programming matrix\n",
    "        # the entry b[k][i] corresponds to the subproblem b_k(i+1)\n",
    "        # where i is a 0-based index. We will not explicitly represent the begin or end states.\n",
    "        # As a result, the initialization at last position sets the backward probability to 1 (0 in log space).\n",
    "        b = matrix(len(self.states), len(char_string))\n",
    "        if not char_string: return b\n",
    "        \n",
    "        # initialization\n",
    "        for ell in range(len(self.states)):\n",
    "            b[ell][len(char_string) - 1] = 0.0\n",
    "\n",
    "        # main fill stage\n",
    "        for i in range(len(char_string) - 2, -1, -1):\n",
    "            for k in range(len(self.states)):\n",
    "                b[k][i] = sum_log_probs(self.log_transition_prob_matrix[k][ell] +\n",
    "                                        self.log_emission_prob_matrix[ell][char_indices[i + 1]] +\n",
    "                                        b[ell][i + 1]\n",
    "                                        for ell in range(len(self.states)))\n",
    "        return b\n",
    "\n",
    "    def log_probability(self, char_string):\n",
    "        \"\"\"Calculates the (natural) log probability (log(P(char_string))) \n",
    "        of an observed sequence given this HMM\"\"\"\n",
    "        f = self.forward_matrix(char_string)\n",
    "        return sum_log_probs(f[k][-1] for k in range(len(self.states)))\n",
    " \n",
    "    def posterior_matrix(self, char_string):\n",
    "        \"\"\"Computes the posterior probability matrix for the given observed sequence.\n",
    "\n",
    "        Args:\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            a matrix (list of lists) with the entry in the kth row and ith column (e.g., m[k][i]) \n",
    "            giving the posterior probability that state k emitted character i, i.e., P(pi_i = k| x)\n",
    "        \"\"\"\n",
    "        f = self.forward_matrix(char_string)\n",
    "        b = self.backward_matrix(char_string)\n",
    "        log_prob_seq = sum_log_probs(f[k][-1] for k in range(len(self.states)))\n",
    "        p = matrix(len(self.states), len(char_string))\n",
    "        for i in range(len(char_string)):\n",
    "            for k in range(len(self.states)):\n",
    "                p[k][i] = math.exp(f[k][i] + b[k][i] - log_prob_seq)\n",
    "        return p\n",
    "\n",
    "    def posterior_decoding_path(self, char_string):\n",
    "        \"\"\"Computes the posterior decoding path of hidden states for the observed sequence.\n",
    "\n",
    "        In the case that multiple states tie for the highest posterior probability\n",
    "        at a given position, the state with the highest index is chosen.\n",
    "        \n",
    "        Args:\n",
    "            char_string: a string representing the sequence of observed characters (X)\n",
    "        Returns:\n",
    "            A string representing a sequence of hidden states.\n",
    "        \"\"\"\n",
    "        p = self.posterior_matrix(char_string)\n",
    "        state_indices = [max((prob, i) for i, prob in enumerate(col))[1] for col in zip(*p)]\n",
    "        return self.decode_states(state_indices)   \n",
    "### END SOLUTION\n",
    "\n",
    "### BEGIN SOLUTION TEMPLATE=\n",
    "def add_log_probs(log_p, log_q):\n",
    "    \"\"\"Computes the sum of two probabilities in log space.\"\"\"\n",
    "    if log_p < log_q:\n",
    "        log_p, log_q = log_q, log_p\n",
    "    return log_p + math.log(1 + math.exp(log_q - log_p))\n",
    "\n",
    "def sum_log_probs(log_probs):\n",
    "    \"\"\"Computes the sum of an iterable of probabilities in log space\"\"\"\n",
    "    return functools.reduce(add_log_probs, log_probs)\n",
    "### END SOLUTION\n",
    "\n",
    "def sample_categorical(distribution):\n",
    "    \"\"\"Randomly sample from a categorical distribution (a discrete distribution over K categories).\n",
    "    \n",
    "    Args:\n",
    "        distribution: a list of probabilities representing a discrete distribution over K categories.\n",
    "    Returns:\n",
    "        The index of the category sampled.\n",
    "    \"\"\"\n",
    "\n",
    "    r = random.random()\n",
    "    for i, prob in enumerate(distribution):\n",
    "        if r < prob:\n",
    "            return i\n",
    "        else:\n",
    "            r -= prob\n",
    "    # in case we encounter floating point issues return the last index\n",
    "    return len(distribution) - 1    \n",
    "\n",
    "def log_transform_vector(v):\n",
    "    \"\"\"Returns a new vector (a list) with log-transformed values\"\"\"\n",
    "    return [math.log(x) if x != 0 else float(\"-inf\") for x in v]\n",
    "\n",
    "def log_transform_matrix(m):\n",
    "    \"\"\"Returns a new matrix (a list of lists) with log-transformed values\"\"\"\n",
    "    return list(map(log_transform_vector, m))\n",
    "\n",
    "def round_matrix(m, digits=2):\n",
    "    \"\"\"Returns a new matrix (a list of lists) with rounded values\"\"\"\n",
    "    return [round_vector(v, digits) for v in m]\n",
    "    \n",
    "def round_vector(v, digits=2):\n",
    "    \"\"\"Returns a new vector (a list) with rounded values\"\"\"\n",
    "    return [round(x, digits) for x in v]\n",
    "\n",
    "def matrix(num_rows, num_cols, initial_value=None):\n",
    "    \"\"\"Constructs a matrix (a list of lists)\"\"\"\n",
    "    return [[initial_value] * num_cols for i in range(num_rows)]\n",
    "\n",
    "def normalize_vector(v):\n",
    "    \"\"\"Returns a new vector with entries scaled such that the sum of the entries is one.\"\"\"\n",
    "    s = sum(v)\n",
    "    return [x / s for x in v]\n",
    "\n",
    "def normalize_matrix_rows(m):\n",
    "    \"\"\"Returns new matrix with entries scaled such that each row sums to one.\"\"\"\n",
    "    return list(map(normalize_vector, m))\n",
    "\n",
    "def print_matrix(m, precision=3, width=8):\n",
    "    \"\"\"Prints a matrix with values formatted to the given precision and spaced to the given width.\"\"\"\n",
    "    for row in m:\n",
    "        print(''.join(\"{:{}.{}g}\".format(x, width, precision) for x in row))\n",
    "\n",
    "# Using the class above, we construct an HMM for the occasionally dishonest casino example\n",
    "# described in the lecture and textbook\n",
    "casino_states = \"FL\"     # F = fair die, L = loaded die\n",
    "casino_chars = \"123456\"  # the six sides of the die\n",
    "casino_initial_probs = [0.5, 0.5]\n",
    "casino_transition_prob_matrix = [\n",
    "    [0.95, 0.05],\n",
    "    [0.10, 0.90]\n",
    "]\n",
    "\n",
    "casino_emission_prob_matrix = [\n",
    "    [ 1/6,  1/6,  1/6,  1/6,  1/6, 1/6],\n",
    "    [1/10, 1/10, 1/10, 1/10, 1/10, 1/2]\n",
    "]\n",
    "casino_hmm = HiddenMarkovModel(casino_states, \n",
    "                               casino_chars, \n",
    "                               casino_transition_prob_matrix, \n",
    "                               casino_initial_probs,\n",
    "                               casino_emission_prob_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a4506b5d44c94e0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## PROBLEM 1: Estimating parameters of an HMM in the fully observed case (3 POINTS)\n",
    "\n",
    "Implement the `estimate_parameters` method of the `HiddenMarkovModel` class, which computes (optionally smoothed) maximum likelihood estimates for the model given a set of training data in the full observed scenario (i.e., when both the sequences and state paths are given).  You may find the following utility functions, defined above, of use in your implemenation:\n",
    "* `matrix`\n",
    "* `normalize_vector`\n",
    "* `normalize_matrix_rows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce187d8a8d8bebb6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tiny_dataset = [('FFLLLLLFFL', \n",
    "                 '2456166656')]\n",
    "\n",
    "small_dataset = [('FFFFFFFFFF', \n",
    "                  '4416531624'),\n",
    "                 ('FFFFLLLLLL', \n",
    "                  '1245466666'),\n",
    "                 ('LFFFFFFFFF', \n",
    "                  '6553321132'),\n",
    "                 ('FFFLFFFFFF', \n",
    "                  '4544163311'),\n",
    "                 ('LFFFFFFFFF', \n",
    "                  '6541416123')]\n",
    "\n",
    "long_dataset = [\n",
    "    ('FFFFLFFLLLLLFFFFFFLLFFFFFFFFFFFLFFFFLLLFFFFFFFFFFFFFFFFFFLLLLLLLFFFFFFFLLLFLLFFFFFLLFFFFFLLFFLLLLFFF',\n",
    "     '3423612666222146536634655556613623346661122323413226235445665663456631266656664533662121662322666666'),\n",
    "    ('FFFFLFFFFLLLLLLLLLLLFLLLFFLFFFFFLFFFLLLLLLLLLLFLLFFFLLLLLFFFFFFFFFFFFLFLLLFFFFFFFLLLLFLLLLFFFFFFLFFF',\n",
    "     '4424361422656666662616461565531162256646151266424444656664141651441416446655453526616166635313246162'),\n",
    "    ('FFFFFFFFFFFFFFFFLLLFFFFFFFFFFFFFFLFFLLLLLLLLLLLLLFFFFFFFFFLLLLFFLLLFFFLLLLFFLLLLFFFFFFFFFLLFFFLLLLLL',\n",
    "     '1151666324121112666245665164434166456662666616166632324144662615665216366664466366526251266143446666'),\n",
    "    ('LFFFFFFFLLFFFFFFFFFFFLFFFFLLLLLFLFFLFFLFFFFFLLLLLFFFFLLFFFLFFFFFFFFLLFFLLLLLFLLLLLLLFFFFLLLLFFFFFFFL',\n",
    "     '6511631166663446212416411166666465363614122562566314161641634423623632666166256154662335363613226414'),\n",
    "    ('LLLLFFFFFFFFFLFFFLLFFLFFFFFLLLFFFFFLFFFFFLLLFLLLFFFLLLLFFLFLLLLLLLFFFFLLLLLLLLLFFFFFFLLFFFFLLFFFFLLL',\n",
    "     '6626136241445642566146151343663552163264466113332556666216165666654333265666546611515654236662321633'),\n",
    "    ('FFFFFFFFFFFFFFFLLLLLFFFFFFFFLFFFLLLLLLLLLLLLFFLLLLLFFLLLLLLLFFFFLLLLLLFFFFFFFFFFFFLLLLLLLFFFFFLFFFFF',\n",
    "     '1463624613423236666113353212455365566666666624666666666666661313663366466454525516466664644146362254'),\n",
    "    ('LLLLLLLLLLLLLLFLLLLLLLLLLLLLFFFLLLLLFFFFFFFLLLFLFFFFFLLLLLLFFFFFLFFFLLLLLLLLLLLLFFFFFFLLFFFFFFFFFLLF',\n",
    "     '6666664662646661666466666664156662665312515665364213146646663415514666661616666665316615465415236561'),\n",
    "    ('FFLLLLFFFFLLFFFFLLFFFFFFFFFFFFLLFFFFFFFLLFFFFLFFFLFFFLLFFFLLLFFLLLLLFFFFFFFFFFFFFFFFFFFFFFFFFLLLFFLL',\n",
    "     '5361664234262515665463155244266646514656632436652652326114636146166221536123613646512655325151665466'),\n",
    "    ('LLFFFFLLLFLFFFFLFFFLLLLLLLLLFFFFFFFLLLLFFFFFFFFFFFFLLLLLLLLLFLLLFFLLLLFFFFFFLFFFLLLFFFFFFFFFFFFFFFFL',\n",
    "     '4413665663512626261666566656622453166661245132442236646661663666236656246364636466622131225533644616'),\n",
    "    ('LFLLLFFFLFFLLLLFFLLLLLLFFLLLFFFLLFLLFFLFFFFFFFFFFFFFFFFFFFFFFFFFFLLLLLLFFFFFFFFFLFFFLLLLLFFFFFFFFFLF',\n",
    "     '2664664565163633266656652632566261663464243214633262142164543661166366151256125266446662644662652666')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "estimation_of_initial_probs",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: estimation of initial_probs passed all tests\n"
     ]
    }
   ],
   "source": [
    "# tests for estimation of initial_probs\n",
    "tiny_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "tiny_estimated_casino_hmm.estimate_parameters(tiny_dataset, pseudocount=1)\n",
    "small_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "small_estimated_casino_hmm.estimate_parameters(small_dataset, pseudocount=1)\n",
    "long_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "long_estimated_casino_hmm.estimate_parameters(long_dataset)\n",
    "\n",
    "assert round_vector(tiny_estimated_casino_hmm.initial_probs, 2) == [0.67, 0.33]\n",
    "assert round_vector(small_estimated_casino_hmm.initial_probs, 2) == [0.57, 0.43]\n",
    "assert round_vector(long_estimated_casino_hmm.initial_probs, 2) == [0.5, 0.5]\n",
    "print(\"SUCCESS: estimation of initial_probs passed all tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "estimation_of_transition_prob_matrix",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: estimation of transition_prob_matrix passed all tests\n"
     ]
    }
   ],
   "source": [
    "# tests for estimation of transition_prob_matrix\n",
    "tiny_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "tiny_estimated_casino_hmm.estimate_parameters(tiny_dataset, pseudocount=1)\n",
    "small_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "small_estimated_casino_hmm.estimate_parameters(small_dataset, pseudocount=1)\n",
    "long_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "long_estimated_casino_hmm.estimate_parameters(long_dataset)\n",
    "\n",
    "assert (round_matrix(tiny_estimated_casino_hmm.transition_prob_matrix, 2) == \n",
    "        [[0.5, 0.5], \n",
    "         [0.29, 0.71]])\n",
    "assert (round_matrix(small_estimated_casino_hmm.transition_prob_matrix, 2) == \n",
    "        [[0.92, 0.08],\n",
    "         [0.4,  0.6]])\n",
    "assert (round_matrix(long_estimated_casino_hmm.transition_prob_matrix, 2) == \n",
    "        [[0.81, 0.19], \n",
    "         [0.28, 0.72]])\n",
    "print(\"SUCCESS: estimation of transition_prob_matrix passed all tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "estimation_of_emission_prob_matrix",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: estimation of emission_prob_matrix passed all tests\n"
     ]
    }
   ],
   "source": [
    "# tests for estimation of emission_prob_matrix\n",
    "tiny_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "tiny_estimated_casino_hmm.estimate_parameters(tiny_dataset, pseudocount=1)\n",
    "small_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "small_estimated_casino_hmm.estimate_parameters(small_dataset, pseudocount=1)\n",
    "long_estimated_casino_hmm = HiddenMarkovModel(casino_states, casino_chars)\n",
    "long_estimated_casino_hmm.estimate_parameters(long_dataset)\n",
    "\n",
    "assert (round_matrix(tiny_estimated_casino_hmm.emission_prob_matrix, 2) == \n",
    "        [[0.1, 0.2, 0.1, 0.2, 0.2, 0.2], \n",
    "         [0.17, 0.08, 0.08, 0.08, 0.17, 0.42]])\n",
    "assert (round_matrix(small_estimated_casino_hmm.emission_prob_matrix, 2) == \n",
    "        [[0.26, 0.13, 0.17, 0.19, 0.15, 0.11], \n",
    "         [0.07, 0.07, 0.07, 0.2, 0.07, 0.53]])\n",
    "assert (round_matrix(long_estimated_casino_hmm.emission_prob_matrix, 2) ==\n",
    "        [[0.19, 0.17, 0.15, 0.18, 0.14, 0.16], \n",
    "         [0.05, 0.05, 0.05, 0.06, 0.06, 0.73]])\n",
    "print(\"SUCCESS: estimation of emission_prob_matrix passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84829098a08f336e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Application activity: Predicting text messages from numeric keyboard entries\n",
    "\n",
    "Back in the \"old\" days, people used to type text messages on their \"dumb\" phones using the numeric keypad.  The numbers 2-9 mapped to the letters of the alphabet and 0 mapped to a space character (see the layout in the image below).  Unfortunately, there are more letters of the alphabet than numbers on the keypad, and so each number generally maps to more than one character.  To specify a particular character, the user would typically have to press the number corresponding to that character multiple times until the correct character was selected.  This was very tedious and it is amazing that we survived this stone age of cell phone technology.\n",
    "\n",
    "![Telephone keypad](phone_keypad.png)\n",
    "\n",
    "An interesting task presented by this system of typing text messages on a phone is to predict the string of characters that the user is attempting to type given a sequence of numbers, where the user only presses the number corresponding to each character once.  For example, given the sequence of numbers 2, 2, 8, we might predict that user was typing the word \"cat\" (although there are other possibilities, such as \"bat\" and \"act\").\n",
    "\n",
    "This task maps well to a hidden Markov model, where the characters that the user is trying to type are the \"hidden\" states and the observed sequence is the sequence of numbers pressed.  In this part of the activity, we will train such an HMM using a set of real text messages, and see how well it does in predicting text messages given numeric keypad sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text message data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-764b59260754eb90",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "I have downloaded and cleaned a dataset of SMS text messages from the [SMS Spam Collection](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).  For each text message, I have computed the numeric sequence corresponding to it as the \"observed\" sequence.  The text messages are partitioned into a large training set, with which we can estimate parameters of an HMM, and a test set, on which we can analyze the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28f56b015ec908ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def read_text_message_set(filename):\n",
    "    return [line.rstrip().split(\"\\t\") for line in open(filename)]\n",
    "\n",
    "text_message_training_set = read_text_message_set(\"text_message_training_set.txt\")\n",
    "text_message_test_set = read_text_message_set(\"text_message_test_set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00868f5c647acd5e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Your task\n",
    "\n",
    "1. Create an HMM for this task.  The HMM states should be all lowercase letters, numeric digits, and the space character.  The emission characters should be the numeric digits (0-9).\n",
    "2. Train your HMM with the provided training set\n",
    "3. Use for trained HMM and the Viterbi algorithm (or posterior decoding, if you like) to predict on the test set. \n",
    "4. How well does the HMM do in predicting the true messages? Does adding pseudocounts help?\n",
    "\n",
    "*Super bonus activity: Sadly, a basic first-order HMM does not perform too well for this task.  One can improve performance by using a higher-order model, such as a second-order (bigram) HMM.  Construct and train a bigram HMM for this task and see how it performs.  You can use the same HMM class as before, but your states will not be single characters (they should be two-character strings, representing the current character and the previous character); instead of passing a string as the `states` argument to the HMM constructor, pass a list of strings, where each string represents one of the possible bigrams.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TRUTH: oh yeah clearly it s my fault\n",
      "VIT_1ST: ng weai ckeaply it s my datlt\n",
      "VIT_2ND: oh yeah blearly it s my fault\n",
      "\n",
      "  TRUTH: no no i will check all rooms befor activities\n",
      "VIT_1ST: on on i will cheal all sonor bedor caththther\n",
      "VIT_2ND: no on i will check all somor befor cauguither\n",
      "\n",
      "  TRUTH: i ll be in sch fr 4 6 i dun haf da book in sch it s at home\n",
      "VIT_1ST: i ll be in sai fr i m i fto had da anok in sai it s at gome\n",
      "VIT_2ND: i ll be in sch es i m i dun had da cook in sch it s at good\n",
      "\n",
      "  TRUTH: lil fever now fine\n",
      "VIT_1ST: kil dever omy find\n",
      "VIT_2ND: lik dever now dine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION TEMPLATE=Construct/train/predict with/asses your HMM here\n",
    "import string\n",
    "\n",
    "# Construct a first-order HMM\n",
    "text_message_states = string.ascii_lowercase + string.digits + ' '\n",
    "text_message_emissions = string.digits\n",
    "text_message_hmm = HiddenMarkovModel(text_message_states, text_message_emissions)\n",
    "text_message_hmm.estimate_parameters(text_message_training_set, pseudocount=0.00001)\n",
    "\n",
    "# Construct a second-order HMM\n",
    "begin_sentinel = \"^\"\n",
    "second_order_text_message_states = [c1 + c2 \n",
    "                                    for c1 in (begin_sentinel + text_message_states) \n",
    "                                    for c2 in text_message_states]\n",
    "second_order_text_message_hmm = HiddenMarkovModel(second_order_text_message_states, \n",
    "                                                  text_message_emissions)\n",
    "\n",
    "# Functions for converting between strings and bigrams and vice versa\n",
    "def bigrams(s):\n",
    "    return [''.join(pair) for pair in zip(begin_sentinel + s, s)]\n",
    "\n",
    "def unbigram(s):\n",
    "    return ''.join([s[1::2]])\n",
    "\n",
    "bigram_training_set = [(bigrams(text), numbers) for text, numbers in text_message_training_set]\n",
    "second_order_text_message_hmm.estimate_parameters(bigram_training_set, pseudocount=0.00001)\n",
    "\n",
    "# Run the two HMMs on the test set\n",
    "# NOTE: the 2nd-order HMM is very slow in running Viterbi because the HMM class\n",
    "#       implements full-connected states instead of the sparse connections needed in a 2nd-order model\n",
    "for text, numbers in text_message_test_set[:4]:\n",
    "    first_order_viterbi = text_message_hmm.most_probable_path(numbers)\n",
    "    second_order_viterbi = unbigram(second_order_text_message_hmm.most_probable_path(numbers))\n",
    "    print(\"  TRUTH:\", text)\n",
    "    print(\"VIT_1ST:\", first_order_viterbi)\n",
    "    print(\"VIT_2ND:\", second_order_viterbi)\n",
    "    print()\n",
    "\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
